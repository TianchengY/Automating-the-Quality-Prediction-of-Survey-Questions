{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the provided Excel file\n",
    "data = pd.read_csv(\"SQP_database_for_meta_analysis_20240111_comma.csv\")\n",
    "\n",
    "# Define numerical features as provided\n",
    "numerical_features = [\n",
    "    'Number of categories', 'Maximum possible value_75', 'Maximum possible value_76',\n",
    "    'Number of fixed reference points', 'Number of sentences in introduction',\n",
    "    'Number of words in introduction', 'Number of subordinated clauses in introduction',\n",
    "    'Number of sentences in the request', 'Number of words in request',\n",
    "    'Total number of nouns in request for an answer',\n",
    "    'Total number of abstract nouns in request for an answer',\n",
    "    'Total number of syllables in request', 'Number of subordinate clauses in request',\n",
    "    'Number of syllables in answer scale', 'Total number of nouns in answer scale',\n",
    "    'Total number of abstract nouns in answer scale', 'Position'\n",
    "]\n",
    "\n",
    "# Remove rows where 'quality(q^2)' has missing values\n",
    "data_cleaned = data.dropna(subset=['quality(q^2)'])\n",
    "\n",
    "# For numerical features, fill missing values with 0\n",
    "data_cleaned[numerical_features] = data_cleaned[numerical_features].fillna(0)\n",
    "\n",
    "# Identify categorical features (all columns from 'Domain' to 'Position' not listed as numerical)\n",
    "feature_columns = data_cleaned.loc[:, 'Domain':'Position'].columns.tolist()\n",
    "categorical_features = [col for col in feature_columns if col not in numerical_features]\n",
    "\n",
    "# Convert categorical features to dummy variables\n",
    "# Note: Missing values in these features will naturally result in zeros for all dummy variables\n",
    "df_dummies = pd.get_dummies(data_cleaned, columns=categorical_features, dummy_na=False,dtype=float)\n",
    "\n",
    "for column in feature_columns:\n",
    "    original_column = data_cleaned[column].copy()\n",
    "    df_dummies[column+'_ori'] = original_column\n",
    "\n",
    "# Save the processed data to a new Excel file\n",
    "processed_file_path = 'SQP_dummyvars_data.csv'\n",
    "df_dummies.to_csv(processed_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df_dummies['quality(q^2)'].astype(float).astype(int)\n",
    "df_dummies['quality(q^2)'] = col.where(col == 1, ('0.' + col.astype(str)).astype(float))\n",
    "\n",
    "processed_file_path = 'SQP_dummyvars_data.xlsx'\n",
    "df_dummies.to_excel(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be6cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "data = pd.read_excel('SQP_dummyvars_data.xlsx')\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = [\n",
    "    'Number of categories', 'Maximum possible value_75', 'Maximum possible value_76',\n",
    "    'Number of fixed reference points', 'Number of sentences in introduction',\n",
    "    'Number of words in introduction', 'Number of subordinated clauses in introduction',\n",
    "    'Number of sentences in the request', 'Number of words in request',\n",
    "    'Total number of nouns in request for an answer',\n",
    "    'Total number of abstract nouns in request for an answer',\n",
    "    'Total number of syllables in request', 'Number of subordinate clauses in request',\n",
    "    'Number of syllables in answer scale', 'Total number of nouns in answer scale',\n",
    "    'Total number of abstract nouns in answer scale'\n",
    "]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_feature_start = data.columns.get_loc('Domain_1')\n",
    "categorical_feature_end = data.columns.get_loc('Visual or oral presentation_1') + 1\n",
    "# categorical_feature_end = data.columns.get_loc('Request present in the introduction_1.0') + 1\n",
    "categorical_features = data.columns[categorical_feature_start:categorical_feature_end]\n",
    "\n",
    "# Combine all feature columns\n",
    "all_features = numerical_features + list(categorical_features)\n",
    "\n",
    "# Define the target and input features\n",
    "X = data[all_features]\n",
    "y = data['quality(q^2)']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to PyTorch tensors and transfer to GPU if available\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float).view(-1, 1).to(device)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(len(all_features), 256)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc0(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Initialize the network, transfer it to GPU, define loss function and optimizer\n",
    "model = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10000\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=0)\n",
    "\n",
    "\n",
    "# Create a DataLoader instance for batch processing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b200f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# model_best = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8bf3b",
   "metadata": {},
   "source": [
    "## without external features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c372e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_test_loss = 1\n",
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        if test_loss < best_test_loss:\n",
    "            model_best = copy.deepcopy(model)\n",
    "            best_test_loss = test_loss\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42723d",
   "metadata": {},
   "source": [
    "## without position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6660c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_test_loss = 1\n",
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        if test_loss < best_test_loss:\n",
    "            model_best = copy.deepcopy(model)\n",
    "            best_test_loss = test_loss\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb626a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = model_best(X_test_tensor).cpu()\n",
    "test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs.detach())\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "residuals_train = y_train_tensor.cpu().detach().numpy() - model_best(X_train_tensor).cpu().detach().numpy()\n",
    "residuals_test = y_test_tensor.cpu().detach().numpy() - test_outputs.detach().numpy()\n",
    "\n",
    "std_dev_train = np.std(residuals_train)\n",
    "std_dev_test = np.std(residuals_test)\n",
    "\n",
    "threshold_train = 3 * std_dev_train\n",
    "threshold_test = 3 * std_dev_test\n",
    "\n",
    "outliers_train = np.where(np.abs(residuals_train) > threshold_train)[0]\n",
    "print(\"Training set outliers:\", outliers_train)\n",
    "\n",
    "outliers_test = np.where(np.abs(residuals_test) > threshold_test)[0]\n",
    "print(\"Test set outliers:\", outliers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07706bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = X_train.index\n",
    "test_indices = X_test.index\n",
    "\n",
    "outlier_indices_train = train_indices[outliers_train]\n",
    "outlier_indices_test = test_indices[outliers_test]\n",
    "\n",
    "outlier_details_train = data.loc[outlier_indices_train, :]\n",
    "outlier_details_test = data.loc[outlier_indices_test, :]\n",
    "\n",
    "# print(\"Training set outlier details:\")\n",
    "# print(outlier_details_train)\n",
    "# print(\"\\nTest set outlier details:\")\n",
    "# print(outlier_details_test)\n",
    "outlier_details_train.to_csv(\"outliers/nn_quality_train.csv\")\n",
    "outlier_details_test.to_csv(\"outliers/nn_quality_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_details_train['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dff062",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_details_test['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdf044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811791a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7aac2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_train_tensor).cpu()\n",
    "    test_loss = mean_squared_error(y_train_tensor.cpu(), test_outputs)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83ee89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "data = pd.read_excel('SQP_dummyvars_data.xlsx')\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = [\n",
    "    'Number of categories', 'Maximum possible value_75', 'Maximum possible value_76',\n",
    "    'Number of fixed reference points', 'Number of sentences in introduction',\n",
    "    'Number of words in introduction', 'Number of subordinated clauses in introduction',\n",
    "    'Number of sentences in the request', 'Number of words in request',\n",
    "    'Total number of nouns in request for an answer',\n",
    "    'Total number of abstract nouns in request for an answer',\n",
    "    'Total number of syllables in request', 'Number of subordinate clauses in request',\n",
    "    'Number of syllables in answer scale', 'Total number of nouns in answer scale',\n",
    "    'Total number of abstract nouns in answer scale', 'Position'\n",
    "]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_feature_start = data.columns.get_loc('Domain_1')\n",
    "categorical_feature_end = data.columns.get_loc('Visual or oral presentation_1') + 1\n",
    "categorical_features = data.columns[categorical_feature_start:categorical_feature_end]\n",
    "\n",
    "# Combine all feature columns\n",
    "all_features = numerical_features + list(categorical_features)\n",
    "\n",
    "# Define the target and input features\n",
    "X = data[all_features].values\n",
    "y = data['quality(q^2)'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to PyTorch tensors and transfer to GPU if available\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float).view(-1, 1).to(device)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(len(all_features), 256)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc0(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Initialize the network, transfer it to GPU, define loss function and optimizer\n",
    "model = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs/10, eta_min=0)\n",
    "\n",
    "\n",
    "# Create a DataLoader instance for batch processing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd326b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 996/1000, Loss: 0.009784822352230549, Test MSE: 0.014666990377008915\n",
      "Epoch 997/1000, Loss: 0.019383227452635765, Test MSE: 0.01455196738243103\n",
      "Epoch 998/1000, Loss: 0.007361909840255976, Test MSE: 0.014125244691967964\n",
      "Epoch 999/1000, Loss: 0.005996024236083031, Test MSE: 0.015218758955597878\n",
      "Epoch 1000/1000, Loss: 0.012332906015217304, Test MSE: 0.014809948392212391\n"
     ]
    }
   ],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f172741",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccae401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f36014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613c55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c9ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).cpu()\n",
    "        test_loss = mean_squared_error(y_test_tensor.cpu(), test_outputs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test MSE: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a45015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('outliers/mbert_dummy_quality_train.csv', index_col=0)\n",
    "df2 = pd.read_csv('outliers/nn_quality_train.csv', index_col=0)\n",
    "df3 = pd.read_csv('outliers/rf_quality_train.csv', index_col=0)\n",
    "\n",
    "merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "final_merged_df = pd.merge(merged_df, df3, left_index=True, right_index=True)\n",
    "\n",
    "print(final_merged_df)\n",
    "\n",
    "final_merged_df.to_csv('outliers/common_train_outlier.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f99cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('SQP_dummyvars_data.xlsx')\n",
    "data['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c65dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7376b2",
   "metadata": {},
   "source": [
    "# Random Forest + BERT top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data and preprocess\n",
    "data = pd.read_excel(\"SQP_dummyvars_data.xlsx\")\n",
    "data.fillna(-1, inplace=True)\n",
    "features = data.iloc[:, data.columns.get_loc(\"Domain_ori\"):data.columns.get_loc(\"Position_ori\")]\n",
    "labels = data[\"quality(q^2)\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define dataset for handling text data\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize tokenizer and datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "train_texts = (data.loc[X_train.index, \"Request for answer text\"].tolist(), \n",
    "               data.loc[X_train.index, \"Answer options text\"].tolist())\n",
    "test_texts = (data.loc[X_test.index, \"Request for answer text\"].tolist(), \n",
    "              data.loc[X_test.index, \"Answer options text\"].tolist())\n",
    "train_dataset = QualityDataset(tokenizer, train_texts, y_train.tolist())\n",
    "test_dataset = QualityDataset(tokenizer, test_texts, y_test.tolist())\n",
    "\n",
    "# Generate embeddings using BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertModel.from_pretrained('bert-base-multilingual-cased').to(device)\n",
    "\n",
    "def generate_bert_embeddings(dataset):\n",
    "    bert_model.eval()\n",
    "    embeddings = []\n",
    "    for data in tqdm(dataset, desc=\"Generating BERT embeddings\"):\n",
    "        inputs = {key: val.to(device) for key, val in data.items() if key != 'labels'}\n",
    "        inputs = {key: val.unsqueeze(0) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            cls_embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings[0])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "train_embeddings = generate_bert_embeddings(train_dataset)\n",
    "test_embeddings = generate_bert_embeddings(test_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on BERT embeddings only and select top 50 features\n",
    "rf_model_bert_only = RandomForestRegressor(n_estimators=520, max_depth=50,max_features=50,n_jobs=-1, random_state=42)\n",
    "rf_model_bert_only.fit(train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(rf_model_bert_only, max_features=50, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_reduced = selector.transform(train_embeddings)\n",
    "test_embeddings_reduced = selector.transform(test_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981daa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine BERT top 50 features with original features\n",
    "X_train_combined = np.concatenate((X_train.values, train_embeddings_reduced), axis=1)\n",
    "X_test_combined = np.concatenate((X_test.values, test_embeddings_reduced), axis=1)\n",
    "\n",
    "# Train and evaluate Random Forest on the combined feature set\n",
    "rf_model_combined = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16,n_jobs=-1, random_state=42)\n",
    "rf_model_combined.fit(X_train_combined, y_train)\n",
    "predictions_combined = rf_model_combined.predict(X_test_combined)\n",
    "mse_combined = mean_squared_error(y_test, predictions_combined)\n",
    "\n",
    "print(\"MSE with combined features:\", mse_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16,n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e48425",
   "metadata": {},
   "source": [
    "## Iteratively select top feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce features by half based on feature importances\n",
    "def reduce_features_by_half(model, train_embeddings, test_embeddings):\n",
    "    # Get feature importances and select the top 50%\n",
    "    selector = SelectFromModel(model, threshold=\"median\", prefit=True)\n",
    "    train_embeddings_reduced = selector.transform(train_embeddings)\n",
    "    test_embeddings_reduced = selector.transform(test_embeddings)\n",
    "    return train_embeddings_reduced, test_embeddings_reduced\n",
    "\n",
    "# Initialize embeddings for the reduction process\n",
    "train_embeddings_iter = train_embeddings\n",
    "test_embeddings_iter = test_embeddings\n",
    "\n",
    "# Iteratively reduce features and update embeddings\n",
    "for i in range(4):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    # Train Random Forest on current set of embeddings\n",
    "    rf_model_iter = RandomForestRegressor(n_estimators=520, max_depth=50,n_jobs=-1,max_features=50, random_state=42)\n",
    "    rf_model_iter.fit(train_embeddings_iter, y_train)\n",
    "    \n",
    "    # Reduce features by half\n",
    "    train_embeddings_iter, test_embeddings_iter = reduce_features_by_half(rf_model_iter, train_embeddings_iter, test_embeddings_iter)\n",
    "    \n",
    "    # Output the number of features remaining after this iteration\n",
    "    print(f\"Number of features after iteration {i+1}: {train_embeddings_iter.shape[1]}\")\n",
    "\n",
    "# Combine the final set of reduced BERT embeddings with original features\n",
    "X_train_combined = np.concatenate((X_train.values, train_embeddings_iter), axis=1)\n",
    "X_test_combined = np.concatenate((X_test.values, test_embeddings_iter), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest on the combined feature set\n",
    "rf_model_combined = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16,n_jobs=-1, random_state=42)\n",
    "rf_model_combined.fit(X_train_combined, y_train)\n",
    "predictions_combined = rf_model_combined.predict(X_test_combined)\n",
    "mse_combined = mean_squared_error(y_test, predictions_combined)\n",
    "\n",
    "print(\"Final MSE with iteratively reduced and combined features:\", mse_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25414fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee465c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288ca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
