{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel(\"SQP_dummyvars_data.xlsx\")\n",
    "data = data.dropna(subset=[\"quality(q^2)\"])\n",
    "features = data[[\"Request for answer text\", \"Answer options text\"]]\n",
    "labels_quality = data[\"quality(q^2)\"]\n",
    "labels_reliability = data[\"reliability(r^2)\"]\n",
    "labels_validity = data[\"validity(v^2)\"]\n",
    "\n",
    "train_features, test_features, train_labels_quality, test_labels_quality, train_labels_reliability, test_labels_reliability, train_labels_validity, test_labels_validity = train_test_split(\n",
    "    features, labels_quality, labels_reliability, labels_validity, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# # Prepare the datasets\n",
    "# train_dataset_reliability = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_reliability.tolist())\n",
    "# test_dataset_reliability = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_reliability.tolist())\n",
    "\n",
    "# train_dataset_validity = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_validity.tolist())\n",
    "# test_dataset_validity = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_validity.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312efea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Function to predict and save results with MSE\n",
    "def predict_and_save(trainer, dataset, prefix):\n",
    "    predictions = trainer.predict(dataset).predictions.squeeze()\n",
    "    labels = dataset.labels\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"Labels\": labels,\n",
    "        \"Predictions\": predictions\n",
    "    })\n",
    "    results.to_csv(f\"{prefix}_predictions_{mse:.4f}.csv\", index=False)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# # Redefine compute_mse to predict and save results for both train and test datasets\n",
    "# def compute_mse(trainer, train_dataset, test_dataset):\n",
    "#     train_mse = predict_and_save(trainer, train_dataset, \"train\")\n",
    "#     test_mse = predict_and_save(trainer, test_dataset, \"test\")\n",
    "    \n",
    "#     return {\"train_mse\": train_mse, \"test_mse\": test_mse}\n",
    "\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "class EarlyStoppingByMSE(EarlyStoppingCallback):\n",
    "    def __init__(self, patience=1, min_delta=0.0, verbose=False):\n",
    "        super().__init__(patience, min_delta, verbose)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        logs = kwargs.get(\"metrics\", {})\n",
    "        mse = logs.get(\"eval_loss\")  # 假设eval_loss是MSE\n",
    "\n",
    "        if mse is not None and mse < 0.015:\n",
    "            control.should_training_stop = True\n",
    "            if self.verbose:\n",
    "                print(f\"Early stopping as MSE reached {mse:.4f}, which is below the threshold of 0.15.\")\n",
    "        return control\n",
    "\n",
    "# Use this function in the Trainer evaluation\n",
    "# Note: You will need to pass the trainer, train_dataset, and test_dataset to this function\n",
    "\n",
    "# Training arguments (remains unchanged)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define the datasets (remains unchanged)\n",
    "train_dataset_reliability = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_reliability.tolist())\n",
    "test_dataset_reliability = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_reliability.tolist())\n",
    "\n",
    "train_dataset_validity = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_validity.tolist())\n",
    "test_dataset_validity = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_validity.tolist())\n",
    "\n",
    "# Load the models (remains unchanged)\n",
    "model_reliability = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "model_validity = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainers\n",
    "trainer_reliability = Trainer(\n",
    "    model=model_reliability,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_reliability,\n",
    "    eval_dataset=test_dataset_reliability,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "trainer_validity = Trainer(\n",
    "    model=model_validity,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_validity,\n",
    "    eval_dataset=test_dataset_validity,\n",
    "    compute_metrics=compute_mse,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training reliability model\n",
    "trainer_reliability.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254aeaa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training validity model\n",
    "trainer_validity.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b9811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "train_dataset_validity = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_validity.tolist())\n",
    "test_dataset_validity = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_validity.tolist())\n",
    "\n",
    "# Load the models (remains unchanged)\n",
    "model_validity_bert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=1)\n",
    "\n",
    "trainer_validity_bert = Trainer(\n",
    "    model=model_validity_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_validity,\n",
    "    eval_dataset=test_dataset_validity,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "trainer_validity_bert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4e879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "train_dataset_reliability = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_reliability.tolist())\n",
    "test_dataset_reliability = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_reliability.tolist())\n",
    "\n",
    "# Load the models (remains unchanged)\n",
    "model_reliability_bert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=1)\n",
    "\n",
    "trainer_reliability_bert = Trainer(\n",
    "    model=model_reliability_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_reliability,\n",
    "    eval_dataset=test_dataset_reliability,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "trainer_reliability_bert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d73148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cda6e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset_quality = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_quality.tolist())\n",
    "test_dataset_quality = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_quality.tolist())\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_quality = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer_quality_ro = Trainer(\n",
    "    model=model_quality,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_quality,\n",
    "    eval_dataset=test_dataset_quality,\n",
    "    compute_metrics=compute_mse,\n",
    "    #callbacks=[EarlyStoppingByMSE(patience=1, verbose=True)],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_quality_ro.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed29548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset_quality = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_quality.tolist())\n",
    "test_dataset_quality = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_quality.tolist())\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_quality_bert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer_quality = Trainer(\n",
    "    model=model_quality_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_quality,\n",
    "    eval_dataset=test_dataset_quality,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_quality.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b84df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_quality.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6487d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-multilingual-cased')\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset_quality = QualityDataset(tokenizer, (train_features[\"Request for answer text\"].tolist(), train_features[\"Answer options text\"].tolist()), train_labels_quality.tolist())\n",
    "test_dataset_quality = QualityDataset(tokenizer, (test_features[\"Request for answer text\"].tolist(), test_features[\"Answer options text\"].tolist()), test_labels_quality.tolist())\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_quality_bert = BertForSequenceClassification.from_pretrained('bert-large-multilingual-cased', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer_quality = Trainer(\n",
    "    model=model_quality_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_quality,\n",
    "    eval_dataset=test_dataset_quality,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_quality.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba164e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_predictions_test = trainer_reliability_bert.predict(test_dataset_reliability).predictions\n",
    "\n",
    "validity_predictions_test = trainer_validity_bert.predict(test_dataset_validity).predictions\n",
    "\n",
    "quality_predictions_test = trainer_quality.predict(test_dataset_quality).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b761a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dense neural network model\n",
    "class QualityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QualityModel, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# Prepare input for the dense neural network\n",
    "train_inputs = np.concatenate((\n",
    "    train_labels_reliability.values.reshape(-1, 1),\n",
    "    train_labels_validity.values.reshape(-1, 1),\n",
    "    (train_labels_reliability.values * train_labels_validity.values).reshape(-1, 1),\n",
    "    train_labels_quality#.values.reshape(-1, 1)\n",
    "), axis=1)\n",
    "\n",
    "test_inputs = np.concatenate((\n",
    "    reliability_predictions_test.reshape(-1, 1),\n",
    "    validity_predictions_test.reshape(-1, 1),\n",
    "    (reliability_predictions_test * validity_predictions_test).reshape(-1, 1),\n",
    "    quality_predictions_test.reshape(-1, 1)\n",
    "), axis=1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n",
    "# train_labels_quality = torch.tensor(train_labels_quality.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs, dtype=torch.float32)\n",
    "# test_labels_quality = torch.tensor(test_labels_quality.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Initialize and train the dense neural network\n",
    "quality_model = QualityModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(quality_model.parameters(), lr=0.00005)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    quality_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = quality_model(train_inputs)\n",
    "    loss = criterion(outputs, train_labels_quality)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    quality_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = quality_model(test_inputs)\n",
    "        test_loss = criterion(test_outputs, test_labels_quality).item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test MSE: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c426058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final quality model\n",
    "quality_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = quality_model(test_inputs)\n",
    "    mse = mean_squared_error(test_labels_quality.numpy(), predictions.numpy())\n",
    "    print(f\"MSE on test set: {mse:.4f}\")\n",
    "\n",
    "# Save the predictions and MSE to a file\n",
    "results = pd.DataFrame({\n",
    "    \"Test Labels Quality\": test_labels_quality.numpy().flatten(),\n",
    "    \"Predictions\": predictions.numpy().flatten()\n",
    "})\n",
    "results.to_csv(f\"quality_predictions_{mse:.4f}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the models\n",
    "model_reliability = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "model_validity = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainers\n",
    "trainer_reliability = Trainer(\n",
    "    model=model_reliability,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_reliability,\n",
    "    eval_dataset=test_dataset_reliability,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "trainer_validity = Trainer(\n",
    "    model=model_validity,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_validity,\n",
    "    eval_dataset=test_dataset_validity,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training reliability model\n",
    "trainer_reliability.train()\n",
    "# Evaluate reliability model\n",
    "reliability_predictions = trainer_reliability.predict(test_dataset_reliability).predictions\n",
    "\n",
    "# Start training validity model\n",
    "trainer_validity.train()\n",
    "# Evaluate validity model\n",
    "validity_predictions = trainer_validity.predict(test_dataset_validity).predictions\n",
    "\n",
    "# Define the dense neural network model\n",
    "class QualityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QualityModel, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(130, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# Prepare input for the dense neural network\n",
    "train_inputs = np.concatenate((train_labels_reliability.values.reshape(-1, 1), train_labels_validity.values.reshape(-1, 1)), axis=1)\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n",
    "train_labels_quality = torch.tensor(train_labels_quality.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "test_inputs = np.concatenate((reliability_predictions, validity_predictions), axis=1)\n",
    "test_inputs = torch.tensor(test_inputs, dtype=torch.float32)\n",
    "test_labels_quality = torch.tensor(test_labels_quality.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Initialize and train the dense neural network\n",
    "quality_model = QualityModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(quality_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    quality_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = quality_model(train_inputs)\n",
    "    loss = criterion(outputs, train_labels_quality)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the final quality model\n",
    "quality_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = quality_model(test_inputs)\n",
    "    mse = mean_squared_error(test_labels_quality.numpy(), predictions.numpy())\n",
    "    print(f\"MSE on test set: {mse:.4f}\")\n",
    "\n",
    "# Save the predictions and MSE to a file\n",
    "results = pd.DataFrame({\n",
    "    \"Test Labels Quality\": test_labels_quality.numpy().flatten(),\n",
    "    \"Predictions\": predictions.numpy().flatten()\n",
    "})\n",
    "results.to_csv(f\"quality_predictions_{mse:.4f}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed52b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_questions=[\"\"\"Now I have a couple of statements about keeping up with news of current affairs.\n",
    "“I’m doing a better job now than I was a year ago at keeping up with news about current affairs.” \n",
    "“I’m doing a worse job now than I was a year ago at keeping up with news about current affairs.”\n",
    "Which of these represents your situation, or are you somewhere in between?\n",
    " Would you say that you are doing a lot better or just somewhat better at keeping up with the news about current affairs?\"\"\",\n",
    "               \"\"\"“In the country as a whole, business conditions are better now than they were a year ago.”\n",
    "“In the country as a whole, business conditions are worse now than they were a year ago.\n",
    "Which if these represents your opinion, or are you somwhere in between?\n",
    "Would you say that business conditions are a lot better, or just somewhat better?\n",
    "\n",
    "               \"\"\"]\n",
    "\n",
    "good_answers=[\"\"\"1. A lot better\n",
    "2. Somewhat better\n",
    "3. A bit better\n",
    "4. Hasn’t changed\n",
    "5. A bit worse \n",
    "6. Somewhat worse \n",
    "7. A lot worse\"\"\",\n",
    "              \"\"\"1. A lot better\n",
    "2. Somewhat better\n",
    "3. A bit better\n",
    "4. Haven’t changed\n",
    "5. A bit worse\n",
    "6. Somewhat worse\n",
    "7. A lot worse\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c917d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_questions=[\"\"\"Now I have a couple of statements about keeping up with news of current affairs.\n",
    "“I’m doing a better job now than I was a year ago at keeping up with news about current affairs.” \n",
    "“I’m doing a worse job now than I was a year ago at keeping up with news about current affairs.”\n",
    "Which of these represents your situation, or are you somewhere in between?\n",
    " Would you say that you are doing a lot better or just somewhat better at keeping up with the news about current affairs?\"\"\",\n",
    "               \"\"\"“In the country as a whole, business conditions are better now than they were a year ago.”\n",
    "“In the country as a whole, business conditions are worse now than they were a year ago.\n",
    "Which if these represents your opinion, or are you somwhere in between?\n",
    "Would you say that business conditions are a lot better, or just somewhat better?\n",
    "\n",
    "               \"\"\"]\n",
    "\n",
    "bad_answers=[\"\"\"1. A lot better\n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. \n",
    "7. A lot worse\"\"\",\n",
    "              \"\"\"1. A lot better\n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. \n",
    "7. A lot worse\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2d96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
