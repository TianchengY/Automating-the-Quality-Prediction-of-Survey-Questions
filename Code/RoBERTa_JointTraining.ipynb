{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer, Trainer, TrainingArguments, PreTrainedModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split by group_id\n",
    "unique_ids = df[\"group_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "train_df['Processed Request for answer text'] = train_df.apply(preprocess_text, axis=1)\n",
    "test_df['Processed Request for answer text'] = test_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "# ========== Dataset ==========\n",
    "\n",
    "class JointQualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, task=\"regression\"):\n",
    "        self.task = task\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_length = 128\n",
    "        if task == \"pairwise\":\n",
    "            self.pairs = self._make_pairs()\n",
    "\n",
    "    def _make_pairs(self):\n",
    "        pairs = []\n",
    "        for _, group in self.df.groupby([\"Study\", \"Country\", \"ItemConcept\"]):\n",
    "            group = group.reset_index(drop=True)\n",
    "            if len(group) < 2:\n",
    "                continue\n",
    "            for i, j in combinations(range(len(group)), 2):\n",
    "                row_i, row_j = group.loc[i], group.loc[j]\n",
    "                label = 1 if row_i[\"quality(q^2)\"] > row_j[\"quality(q^2)\"] else 0\n",
    "                pairs.append((row_i, row_j, label))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) if self.task == \"regression\" else len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.task == \"regression\":\n",
    "            row = self.df.loc[idx]\n",
    "            text = row[\"Processed Request for answer text\"]\n",
    "            answer = row[\"Answer options text\"]\n",
    "            inputs = self.tokenizer(text, answer, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "            item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "            item['labels'] = torch.tensor(row[\"quality(q^2)\"], dtype=torch.float)\n",
    "            return item\n",
    "        else:\n",
    "            row_i, row_j, label = self.pairs[idx]\n",
    "            text_i = row_i[\"Processed Request for answer text\"]\n",
    "            text_j = row_j[\"Processed Request for answer text\"]\n",
    "            ans_i = row_i[\"Answer options text\"]\n",
    "            ans_j = row_j[\"Answer options text\"]\n",
    "            inputs_i = self.tokenizer(text_i, ans_i, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "            inputs_j = self.tokenizer(text_j, ans_j, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "            item = {\n",
    "                'input_ids_i': inputs_i['input_ids'].squeeze(0),\n",
    "                'attention_mask_i': inputs_i['attention_mask'].squeeze(0),\n",
    "                'input_ids_j': inputs_j['input_ids'].squeeze(0),\n",
    "                'attention_mask_j': inputs_j['attention_mask'].squeeze(0),\n",
    "                'labels': torch.tensor(label, dtype=torch.float)\n",
    "            }\n",
    "            return item\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model ==========\n",
    "\n",
    "class JointRobertaModel(nn.Module):\n",
    "    def __init__(self, model_name=\"xlm-roberta-base\"):\n",
    "        super().__init__()\n",
    "        self.encoder = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.reg_head = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        self.pair_head = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                input_ids_i=None, attention_mask_i=None,\n",
    "                input_ids_j=None, attention_mask_j=None,\n",
    "                labels=None, task=\"regression\"):\n",
    "\n",
    "        if task == \"regression\":\n",
    "            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_output = out.last_hidden_state[:, 0, :]\n",
    "            score = self.reg_head(cls_output).squeeze(-1)\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss = nn.MSELoss()(score, labels)\n",
    "            return {\"loss\": loss, \"logits\": score}\n",
    "\n",
    "        elif task == \"pairwise\":\n",
    "            out_i = self.encoder(input_ids=input_ids_i, attention_mask=attention_mask_i).last_hidden_state[:, 0, :]\n",
    "            out_j = self.encoder(input_ids=input_ids_j, attention_mask=attention_mask_j).last_hidden_state[:, 0, :]\n",
    "            diff = self.pair_head(out_i - out_j).squeeze(-1)\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss = nn.BCEWithLogitsLoss()(diff, labels)\n",
    "            return {\"loss\": loss, \"logits\": diff}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "train_reg_dataset = JointQualityDataset(tokenizer, train_df, task=\"regression\")\n",
    "test_reg_dataset = JointQualityDataset(tokenizer, test_df, task=\"regression\")\n",
    "\n",
    "train_pair_dataset = JointQualityDataset(tokenizer, train_df, task=\"pairwise\")\n",
    "test_pair_dataset = JointQualityDataset(tokenizer, test_df, task=\"pairwise\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_reg_loader = DataLoader(train_reg_dataset, batch_size=16, shuffle=True)\n",
    "train_pair_loader = DataLoader(train_pair_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02389dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointRobertaModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "save_dir = \"./saved_models_joint\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "eval_interval = 300\n",
    "\n",
    "for epoch in range(30):  # num_train_epochs\n",
    "    model.train()\n",
    "    reg_iter = iter(train_reg_loader)\n",
    "    pair_iter = cycle(train_pair_loader)\n",
    "\n",
    "    for step, reg_batch in enumerate(reg_iter):\n",
    "        global_step += 1\n",
    "\n",
    "        # === Regression step ===\n",
    "        for k in ['input_ids', 'attention_mask', 'labels']:\n",
    "            reg_batch[k] = reg_batch[k].to(device)\n",
    "\n",
    "        out_reg = model(\n",
    "            input_ids=reg_batch['input_ids'],\n",
    "            attention_mask=reg_batch['attention_mask'],\n",
    "            labels=reg_batch['labels'],\n",
    "            task=\"regression\"\n",
    "        )\n",
    "        loss_reg = out_reg['loss']\n",
    "\n",
    "        # === Pairwise step ===\n",
    "        pair_batch = next(pair_iter)\n",
    "        for k in pair_batch:\n",
    "            pair_batch[k] = pair_batch[k].to(device)\n",
    "\n",
    "        out_pair = model(\n",
    "            input_ids_i=pair_batch['input_ids_i'],\n",
    "            attention_mask_i=pair_batch['attention_mask_i'],\n",
    "            input_ids_j=pair_batch['input_ids_j'],\n",
    "            attention_mask_j=pair_batch['attention_mask_j'],\n",
    "            labels=pair_batch['labels'],\n",
    "            task=\"pairwise\"\n",
    "        )\n",
    "        loss_pair = out_pair['loss']\n",
    "\n",
    "        # === Total loss + update ===\n",
    "        total_loss = loss_reg + loss_pair\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if global_step % eval_interval == 0:\n",
    "            print(f\"\\n>>> Epoch {epoch+1} | Step {global_step} | Loss_reg: {loss_reg.item():.4f} | Loss_pair: {loss_pair.item():.4f}\")\n",
    "\n",
    "            # === Save model ===\n",
    "            save_path = os.path.join(save_dir, f\"checkpoint_step{global_step}.pt\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"[Saved] Model saved to {save_path}\")\n",
    "\n",
    "            # === Eval on test_df ===\n",
    "            model.eval()\n",
    "            # -- Regression eval --\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in DataLoader(test_reg_dataset, batch_size=32):\n",
    "                    for k in ['input_ids', 'attention_mask', 'labels']:\n",
    "                        batch[k] = batch[k].to(device)\n",
    "                    out = model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        task=\"regression\"\n",
    "                    )\n",
    "                    all_preds.extend(out['logits'].cpu().numpy())\n",
    "                    all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            mse = mean_squared_error(all_labels, all_preds)\n",
    "\n",
    "            # -- Pairwise eval --\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for batch in DataLoader(test_pair_dataset, batch_size=32):\n",
    "                    for k in batch:\n",
    "                        batch[k] = batch[k].to(device)\n",
    "                    out = model(\n",
    "                        input_ids_i=batch[\"input_ids_i\"],\n",
    "                        attention_mask_i=batch[\"attention_mask_i\"],\n",
    "                        input_ids_j=batch[\"input_ids_j\"],\n",
    "                        attention_mask_j=batch[\"attention_mask_j\"],\n",
    "                        task=\"pairwise\"\n",
    "                    )\n",
    "                    probs = torch.sigmoid(out[\"logits\"])\n",
    "                    preds = (probs > 0.5).long()\n",
    "                    labels = batch[\"labels\"].long()\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += len(labels)\n",
    "            acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "            print(f\"[Eval @ step {global_step}] MSE: {mse:.4f} | Pairwise Acc: {acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070832d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "save_dir = \"./saved_models_joint\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "eval_interval = 300\n",
    "\n",
    "for epoch in range(30):  # num_train_epochs\n",
    "    model.train()\n",
    "    reg_iter = iter(train_reg_loader)\n",
    "    pair_iter = cycle(train_pair_loader)\n",
    "\n",
    "    for step, reg_batch in enumerate(reg_iter):\n",
    "        global_step += 1\n",
    "\n",
    "        # === Regression step ===\n",
    "        for k in ['input_ids', 'attention_mask', 'labels']:\n",
    "            reg_batch[k] = reg_batch[k].to(device)\n",
    "\n",
    "        out_reg = model(\n",
    "            input_ids=reg_batch['input_ids'],\n",
    "            attention_mask=reg_batch['attention_mask'],\n",
    "            labels=reg_batch['labels'],\n",
    "            task=\"regression\"\n",
    "        )\n",
    "        loss_reg = out_reg['loss']\n",
    "\n",
    "        # === Pairwise step ===\n",
    "        pair_batch = next(pair_iter)\n",
    "        for k in pair_batch:\n",
    "            pair_batch[k] = pair_batch[k].to(device)\n",
    "\n",
    "        out_pair = model(\n",
    "            input_ids_i=pair_batch['input_ids_i'],\n",
    "            attention_mask_i=pair_batch['attention_mask_i'],\n",
    "            input_ids_j=pair_batch['input_ids_j'],\n",
    "            attention_mask_j=pair_batch['attention_mask_j'],\n",
    "            labels=pair_batch['labels'],\n",
    "            task=\"pairwise\"\n",
    "        )\n",
    "        loss_pair = out_pair['loss']\n",
    "\n",
    "        # === Total loss + update ===\n",
    "        total_loss = loss_reg + 0.5 * loss_pair\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if global_step % eval_interval == 0:\n",
    "            print(f\"\\n>>> Epoch {epoch+1} | Step {global_step} | Loss_reg: {loss_reg.item():.4f} | Loss_pair: {loss_pair.item():.4f}\")\n",
    "\n",
    "            # === Save model ===\n",
    "            save_path = os.path.join(save_dir, f\"checkpoint_step{global_step}.pt\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"[Saved] Model saved to {save_path}\")\n",
    "\n",
    "            # === Eval on test_df ===\n",
    "            model.eval()\n",
    "            # -- Regression eval --\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in DataLoader(test_reg_dataset, batch_size=32):\n",
    "                    for k in ['input_ids', 'attention_mask', 'labels']:\n",
    "                        batch[k] = batch[k].to(device)\n",
    "                    out = model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        task=\"regression\"\n",
    "                    )\n",
    "                    all_preds.extend(out['logits'].cpu().numpy())\n",
    "                    all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            mse = mean_squared_error(all_labels, all_preds)\n",
    "\n",
    "            # -- Pairwise eval --\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for batch in DataLoader(test_pair_dataset, batch_size=32):\n",
    "                    for k in batch:\n",
    "                        batch[k] = batch[k].to(device)\n",
    "                    out = model(\n",
    "                        input_ids_i=batch[\"input_ids_i\"],\n",
    "                        attention_mask_i=batch[\"attention_mask_i\"],\n",
    "                        input_ids_j=batch[\"input_ids_j\"],\n",
    "                        attention_mask_j=batch[\"attention_mask_j\"],\n",
    "                        task=\"pairwise\"\n",
    "                    )\n",
    "                    probs = torch.sigmoid(out[\"logits\"])\n",
    "                    preds = (probs > 0.5).long()\n",
    "                    labels = batch[\"labels\"].long()\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += len(labels)\n",
    "            acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "            print(f\"[Eval @ step {global_step}] MSE: {mse:.4f} | Pairwise Acc: {acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdeed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d05509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Combined Training Loop ==========\n",
    "\n",
    "def train_joint_model(train_df, tokenizer, model, num_epochs=5):\n",
    "    reg_dataset = JointQualityDataset(tokenizer, train_df, task=\"regression\")\n",
    "    pair_dataset = JointQualityDataset(tokenizer, train_df, task=\"pairwise\")\n",
    "    reg_loader = torch.utils.data.DataLoader(reg_dataset, batch_size=8, shuffle=True)\n",
    "    pair_loader = torch.utils.data.DataLoader(pair_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for reg_batch, pair_batch in zip(reg_loader, pair_loader):\n",
    "            # Regression step\n",
    "            for k in ['input_ids', 'attention_mask', 'labels']:\n",
    "                reg_batch[k] = reg_batch[k].to(device)\n",
    "            out_reg = model(input_ids=reg_batch['input_ids'],\n",
    "                            attention_mask=reg_batch['attention_mask'],\n",
    "                            labels=reg_batch['labels'],\n",
    "                            task=\"regression\")\n",
    "            loss_reg = out_reg['loss']\n",
    "\n",
    "            # Pairwise step\n",
    "            for k in pair_batch:\n",
    "                pair_batch[k] = pair_batch[k].to(device)\n",
    "            out_pair = model(input_ids_i=pair_batch['input_ids_i'],\n",
    "                             attention_mask_i=pair_batch['attention_mask_i'],\n",
    "                             input_ids_j=pair_batch['input_ids_j'],\n",
    "                             attention_mask_j=pair_batch['attention_mask_j'],\n",
    "                             labels=pair_batch['labels'],\n",
    "                             task=\"pairwise\")\n",
    "            loss_pair = out_pair['loss']\n",
    "\n",
    "            total_loss = loss_reg + 0.5 * loss_pair  # adjust Î» here\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} done. Regression loss: {loss_reg.item():.4f}, Pairwise loss: {loss_pair.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccee02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d651bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
