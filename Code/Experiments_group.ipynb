{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0790d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'SQP_dummyvars_data.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=None)\n",
    "df = data['Sheet1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the experiment labeling for ESS1 to ESS5\n",
    "def label_experiment(row):\n",
    "    if row['ItemAdmin'] in [\"A1\",\"A3\", \"A5\", \"H1\", \"H2\", \"H3\", \"H19\", \"H20\", \"H21\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 media_use\"\n",
    "    elif row['ItemAdmin'] in [\"A8\", \"A9\", \"A10\", \"H10\", \"H11\", \"H12\", \"H28\", \"H29\", \"H30\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 social_trust\"\n",
    "    elif row['ItemAdmin'] in [\"B2\",  \"B3\", \"B4\", \"H4\", \"H5\", \"H6\", \"H22\", \"H23\", \"H24\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 political_efficacy\"\n",
    "    elif row['ItemAdmin'] in [\"B7\", \"B8\", \"B9\", \"H13\", \"H14\", \"H15\", \"H31\", \"H32\", \"H33\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 political_trust\"\n",
    "    elif row['ItemAdmin'] in [\"H7\", \"H8\", \"H9\", \"H25\", \"H26\", \"H27\", \"B30\", \"B31\", \"B32\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 political_satisfaction\"\n",
    "    elif row['ItemAdmin'] in [\"B43\", \"B44\", \"B45\", \"H16\", \"H17\", \"H18\", \"H34\", \"H35\", \"H36\"] and row['Study'] == \"ESS Round 1\":\n",
    "        return \"ESS1 left-right_orientation\"\n",
    "    elif row['ItemAdmin'] in [\"B4\", \"B5\", \"B7\", \"IS25\", \"IS26\", \"IS27\", \"IS38\", \"IS39\", \"IS40\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 political_trust\"\n",
    "    elif row['ItemAdmin'] in [\"B25\", \"B26\", \"B27\", \"IS11\", \"IS12\", \"IS13\", \"IS35\", \"IS36\", \"IS37\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 political_satisfaction\"\n",
    "    elif row['ItemAdmin'] in [\"D25\", \"D26\", \"D27\", \"IS5\", \"IS6\", \"IS7\", \"IS28\", \"IS29\", \"IS30\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 evaluation_of_doctors\"\n",
    "    elif row['ItemAdmin'] in [\"G6\", \"G7\", \"G8\", \"IS8\", \"IS9\", \"IS10\", \"IS22\", \"IS23\", \"IS24\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 gender_inequalities\"\n",
    "    elif row['ItemAdmin'] in [\"G22\", \"G23\", \"G24\", \"IS2\", \"IS3\", \"IS4\", \"IS15\", \"IS16\", \"IS17\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 housework\"\n",
    "    elif row['ItemAdmin'] in [\"G64\", \"G66\", \"G70\", \"IS19\", \"IS20\", \"IS21\", \"IS32\", \"IS33\", \"IS34\"] and row['Study'] == \"ESS Round 2\":\n",
    "        return \"ESS2 current_job\"\n",
    "    elif row['ItemAdmin'] in [\"B35\", \"B36\", \"B37\", \"HS1\", \"HS2\", \"HS3\", \"HS13\", \"HS14\", \"HS15\", \"HS25\", \"HS26\", \"HS27\"] and row['Study'] == \"ESS Round 3\":\n",
    "        return \"ESS3 immigration_perceptions\"\n",
    "    elif row['ItemAdmin'] in [\"B38\", \"B39\", \"B40\", \"HS4\", \"HS5\", \"HS6\", \"HS16\", \"HS17\", \"HS18\", \"HS28\", \"HS29\", \"HS30\"] and row['Study'] == \"ESS Round 3\":\n",
    "        return \"ESS3 evaluation_of_immigration\"\n",
    "    elif row['ItemAdmin'] in [\"E26\", \"E27\", \"E28\", \"HS7\", \"HS8\", \"HS9\", \"HS19\", \"HS20\", \"HS21\", \"HS31\", \"HS32\", \"HS33\"] and row['Study'] == \"ESS Round 3\":\n",
    "        return \"ESS3 eudaimonic_well-being\"\n",
    "    elif row['ItemAdmin'] in [\"E40\", \"E43\", \"E45\", \"HS10\", \"HS11\", \"HS12\", \"HS22\", \"HS23\", \"HS24\", \"HS34\", \"HS35\", \"HS36\"] and row['Study'] == \"ESS Round 3\":\n",
    "        return \"ESS3 life_satisfaction\"\n",
    "    elif row['ItemAdmin'] in [\"A1\", \"A3\", \"A5\", \"HS1\", \"HS2\", \"HS3\", \"HS13\", \"HS14\", \"HS15\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 media_use\"\n",
    "    elif row['ItemAdmin'] in [\"A8\", \"A9\", \"HS4\", \"HS5\", \"HS6\", \"HS25\", \"HS26\", \"HS27\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 social_trust\"\n",
    "    elif row['ItemAdmin'] in [\"B4\", \"B5\", \"B6\", \"HS16\", \"HS17\", \"HS18\", \"HS28\", \"HS29\", \"HS30\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 political_trust\"\n",
    "    elif row['ItemAdmin'] in [\"B23\", \"HS22\", \"HS23\", \"HS24\", \"HS34\", \"HS35\", \"HS36\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 left-right_placement\"\n",
    "    elif row['ItemAdmin'] in [\"B25\", \"B26\", \"B27\", \"HS7\", \"HS8\", \"HS9\", \"HS19\", \"HS20\", \"HS21\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 political_satisfaction\"\n",
    "    elif row['ItemAdmin'] in [\"B30\", \"B31\", \"HS10\", \"HS11\", \"HS12\", \"HS31\", \"HS32\", \"HS33\"] and row['Study'] == \"ESS Round 4\":\n",
    "        return \"ESS4 left-right_orientation\"\n",
    "    elif row['ItemAdmin'] in [\"D4\", \"D5\", \"D6\", \"I10\", \"I11\", \"I12\", \"I19\", \"I20\", \"I21\"] and row['Study'] == \"ESS Round 5\":\n",
    "        return \"ESS5 effectiveness_of_the_police\"\n",
    "    elif row['ItemAdmin'] in [\"D12\", \"D13\", \"D14\", \"I15\", \"I13\", \"I14\", \"I4\", \"I5\", \"I6\"] and row['Study'] == \"ESS Round 5\":\n",
    "        return \"ESS5 satisfaction_with_the_police\"\n",
    "    elif row['ItemAdmin'] in [\"D15\", \"D17\", \"D16\", \"I7\", \"I8\", \"I9\", \"I16\", \"I17\", \"I18\"] and row['Study'] == \"ESS Round 5\":\n",
    "        return \"ESS5 evaluation_of_the_police\"\n",
    "    # ESS Round 6\n",
    "    elif row['ItemName'] in [\"imbgeco\", \"imueclt\", \"imwbcnt\", \"teste19\", \"teste20\", \"teste21\", \"teste28\", \"teste29\", \"teste30\"] and row['Study'] == \"ESS Round 6\":\n",
    "        return \"ESS6 evaluation_of_immigration\"\n",
    "    elif row['ItemName'] in [\"fltdpr\", \"slprl\", \"fltlnl\", \"teste4\", \"teste5\", \"teste6\", \"teste13\", \"teste14\", \"teste15\", \"teste25\", \"teste26\", \"teste27\", \"teste34\", \"teste35\", \"teste36\"] and row['Study'] == \"ESS Round 6\":\n",
    "        return \"ESS6 feelings_past_week\"\n",
    "    elif row['ItemName'] in [\"tmimdng\", \"tmabdng\", \"tmendng\", \"teste1\", \"teste2\", \"teste3\", \"teste10\", \"teste11\", \"teste12\", \"teste22\", \"teste23\", \"teste24\", \"teste31\", \"teste32\", \"teste33\"] and row['Study'] == \"ESS Round 6\":\n",
    "        return \"ESS6 everyday_life_engagement\"\n",
    "    elif row['ItemName'] in [\"oppcrgvc\",\"medcrgvc\",\"meprinfc\",\"teste7\", \"teste8\", \"teste9\", \"teste16\", \"teste17\", \"teste18\"] and row['Study'] == \"ESS Round 6\":\n",
    "        return \"ESS6 evaluation_of_democracy\"\n",
    "    # ESS Round 7\n",
    "    elif row['ItemName'] in [\"psppsgv\", \"psppipl\", \"ptcpplt\", \"testf4\", \"testf5\", \"testf6\", \"testf13\", \"testf14\", \"testf15\"] and row['Study'] == \"ESS Round 7\":\n",
    "        return \"ESS7 system_responsiveness\"\n",
    "    elif row['ItemName'] in [\"actrolg\", \"cptppol\", \"etapapl\", \"testf7\", \"testf8\", \"testf9\", \"testf16\", \"testf17\", \"testf18\"] and row['Study'] == \"ESS Round 7\":\n",
    "        return \"ESS7 subjective_competence\"\n",
    "    elif row['ItemName'] in [\"qfimlng\", \"qfimwht\", \"qfimcmt\", \"testf1\", \"testf2\", \"testf3\", \"testf10\", \"testf11\", \"testf12\"] and row['Study'] == \"ESS Round 7\":\n",
    "        return \"ESS7 importance_to_immigration\"\n",
    "    return None\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['experiment'] = df.apply(label_experiment, axis=1)\n",
    "\n",
    "# Filter for rows where 'experiment' is not None\n",
    "# filtered_df = df[df['experiment'].notnull()]\n",
    "filtered_df = df[df['experiment'].isnull()]\n",
    "\n",
    "# Display the first few rows of the filtered dataframe\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c96c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22490747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df['Introduction text'] = \"This is the introduction text for the experiment.\"\n",
    "# filtered_df['Request for answer text'] = \"Please provide your answer for the following question.\"\n",
    "# filtered_df['Answer options text'] = \"These are the available answer options.\"\n",
    "\n",
    "# english_df = filtered_df[filtered_df['Language'] == 'English']\n",
    "# # final_df = english_df[['QuestionId', 'UserId', 'UserName', 'Study', 'ItemAdmin', 'ItemName', 'ItemConcept', 'Country', 'Language', 'experiment', 'Introduction text', 'Request for answer text', 'Answer options text', 'quality(q^2)']]\n",
    "\n",
    "\n",
    "# english_df.head()\n",
    "\n",
    "german_df = filtered_df[filtered_df['Language'] == 'German']\n",
    "# final_df = english_df[['QuestionId', 'UserId', 'UserName', 'Study', 'ItemAdmin', 'ItemName', 'ItemConcept', 'Country', 'Language', 'experiment', 'Introduction text', 'Request for answer text', 'Answer options text', 'quality(q^2)']]\n",
    "\n",
    "\n",
    "german_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a04a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_df.to_csv(\"final_exp_nongroup_german.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d18e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"final_exp.csv\")\n",
    "\n",
    "# Filter the data for training and testing based on 'study' column\n",
    "train_data = data[data['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "test_data = data[data['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column\n",
    "train_data = train_data.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data = test_data.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels\n",
    "train_features = train_data[[\"Request for answer text\", \"Answer options text\"]]\n",
    "train_labels = train_data[\"quality(q^2)\"]\n",
    "test_features = test_data[[\"Request for answer text\", \"Answer options text\"]]\n",
    "test_labels = test_data[\"quality(q^2)\"]\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features[\"Request for answer text\"].tolist(), \n",
    "    train_features[\"Answer options text\"].tolist()), train_labels.tolist())\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features[\"Request for answer text\"].tolist(), \n",
    "    test_features[\"Answer options text\"].tolist()), test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583ddf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a97d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predicted values\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Add the predictions to the test_data dataframe\n",
    "test_data['Predicted quality(q^2)'] = predicted_labels\n",
    "\n",
    "# Save the test_data with predictions to a CSV file\n",
    "test_data.to_csv(\"./group_results/test_data_with_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b06a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"final_exp_all.csv\")\n",
    "\n",
    "# Filter the data for training and testing based on 'study' column\n",
    "train_data = data[data['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "test_data = data[data['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Fill missing values with -1\n",
    "train_data.fillna(-1, inplace=True)\n",
    "test_data.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target\n",
    "domain_index = train_data.columns.get_loc(\"Domain_ori\")\n",
    "position_index = train_data.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "train_features = train_data.iloc[:, domain_index:position_index]\n",
    "train_target = train_data['quality(q^2)']\n",
    "test_features = test_data.iloc[:, domain_index:position_index]\n",
    "test_target = test_data['quality(q^2)']\n",
    "\n",
    "# Train-test split is predefined by the 'study' column split, so no need for train_test_split function here\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "rf_model.fit(train_features, train_target)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(test_features)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_target, predictions)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_exp_allgroup_eng.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# df = data['Sheet1']\n",
    "\n",
    "# Fill missing values with -1\n",
    "data.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target\n",
    "domain_index = data.columns.get_loc(\"Domain_ori\")\n",
    "position_index = data.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "features = data.iloc[:, domain_index:position_index]\n",
    "target = data['quality(q^2)']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2144b",
   "metadata": {},
   "source": [
    "# Combined Two Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5ada0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "\n",
    "# Filter the CSV data for training and testing based on 'study' column\n",
    "train_data_csv = data_csv[data_csv['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "test_data_csv = data_csv[data_csv['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for CSV data\n",
    "train_data_csv = train_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data_csv = test_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for Excel data\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "features_excel = data_excel[[\"Request for answer text\", \"Answer options text\"]]\n",
    "labels_excel = data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Split Excel data into training and testing sets\n",
    "train_features_excel, test_features_excel, train_labels_excel, test_labels_excel = train_test_split(\n",
    "    features_excel, labels_excel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef480a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33251d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_csv = QualityDataset(tokenizer, (\n",
    "    test_features_csv[\"Request for answer text\"].tolist(), \n",
    "    test_features_csv[\"Answer options text\"].tolist()), test_labels_csv.tolist())\n",
    "\n",
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(test_dataset_csv)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "mse = mean_squared_error(test_labels_csv, predicted_labels)\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "output_df = test_features_csv.copy()\n",
    "output_df['actual_quality'] = test_labels_csv\n",
    "output_df['predicted_quality'] = predicted_labels\n",
    "output_df.to_csv('./group_results/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_data_csv)\n",
    "\n",
    "# Extract predicted values\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Add the predictions to the test_data dataframe\n",
    "test_data_csv['Predicted quality(q^2)'] = predicted_labels\n",
    "\n",
    "# Save the test_data with predictions to a CSV file\n",
    "test_data_csv.to_csv(\"./group_results/test_data_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e89df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from both CSV files\n",
    "data_csv1 = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "data_csv2 = pd.read_csv(\"final_exp_allgroup_eng.csv\")\n",
    "\n",
    "# Filter the data for training and testing based on 'study' column from the first CSV\n",
    "train_data_csv1 = data_csv1[data_csv1['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "test_data_csv1 = data_csv1[data_csv1['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Fill missing values with -1 for the first CSV data\n",
    "train_data_csv1.fillna(-1, inplace=True)\n",
    "test_data_csv1.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the first CSV data\n",
    "domain_index1 = train_data_csv1.columns.get_loc(\"Domain_ori\")\n",
    "position_index1 = train_data_csv1.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "train_features_csv1 = train_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "train_target_csv1 = train_data_csv1['quality(q^2)']\n",
    "test_features_csv1 = test_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "test_target_csv1 = test_data_csv1['quality(q^2)']\n",
    "\n",
    "# Fill missing values with -1 for the second CSV data\n",
    "data_csv2.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the second CSV data\n",
    "domain_index2 = data_csv2.columns.get_loc(\"Domain_ori\")\n",
    "position_index2 = data_csv2.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "features_csv2 = data_csv2.iloc[:, domain_index2:position_index2]\n",
    "target_csv2 = data_csv2['quality(q^2)']\n",
    "\n",
    "# Split the second CSV data into training and testing sets\n",
    "train_features_csv2, test_features_csv2, train_target_csv2, test_target_csv2 = train_test_split(\n",
    "    features_csv2, target_csv2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv1, train_features_csv2], ignore_index=True)\n",
    "train_target_combined = pd.concat([train_target_csv1, train_target_csv2], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv1, test_features_csv2], ignore_index=True)\n",
    "test_target_combined = pd.concat([test_target_csv1, test_target_csv2], ignore_index=True)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "rf_model.fit(train_features_combined, train_target_combined)\n",
    "\n",
    "# Make predictions on the combined test set\n",
    "predictions = rf_model.predict(test_features_combined)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_target_combined, predictions)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28516d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3436482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv = data_csv[~data_csv['experiment'].isin(['ESS4 political_trust', 'ESS7 subjective_competence'])]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(['ESS4 political_trust', 'ESS7 subjective_competence'])]\n",
    "\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for CSV data\n",
    "train_data_csv = train_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data_csv = test_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for Excel data\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "features_excel = data_excel[[\"Request for answer text\", \"Answer options text\"]]\n",
    "labels_excel = data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Split Excel data into training and testing sets\n",
    "train_features_excel, test_features_excel, train_labels_excel, test_labels_excel = train_test_split(\n",
    "    features_excel, labels_excel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c0e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to preprocess the text based on 'Introduction text' length\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "\n",
    "# Apply preprocessing to the CSV data\n",
    "data_csv['Processed Request for answer text'] = data_csv.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Apply preprocessing to the Excel data\n",
    "data_excel['Processed Request for answer text'] = data_excel.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv = data_csv[~data_csv['experiment'].isin(['ESS1 political_efficacy'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(['ESS1 political_efficacy'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for CSV data\n",
    "train_data_csv = train_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data_csv = test_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for Excel data\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "features_excel = data_excel[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "labels_excel = data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Split Excel data into training and testing sets\n",
    "train_features_excel, test_features_excel, train_labels_excel, test_labels_excel = train_test_split(\n",
    "    features_excel, labels_excel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0434e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_csv = QualityDataset(tokenizer, (\n",
    "    test_features_csv[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_csv[\"Answer options text\"].tolist()), test_labels_csv.tolist())\n",
    "\n",
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(test_dataset_csv)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "mse = mean_squared_error(test_labels_csv, predicted_labels)\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "output_df = test_features_csv.copy()\n",
    "output_df['actual_quality'] = test_labels_csv\n",
    "output_df['predicted_quality'] = predicted_labels\n",
    "output_df.to_csv('./group_results/predictionsESS1 political_efficacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fe744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bbe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4963025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from both CSV files\n",
    "data_csv2 = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "data_csv1 = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# # Filter the data for training and testing based on 'study' column from the first CSV\n",
    "# train_data_csv1 = data_csv1[data_csv1['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "# test_data_csv1 = data_csv1[data_csv1['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv1 = data_csv1[~data_csv1['experiment'].isin(['ESS5 satisfaction_with_the_police'])] #,'ESS1 political_efficacy' 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "test_data_csv1 = data_csv1[data_csv1['experiment'].isin(['ESS5 satisfaction_with_the_police'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "\n",
    "\n",
    "# Fill missing values with -1 for the first CSV data\n",
    "train_data_csv1.fillna(-1, inplace=True)\n",
    "test_data_csv1.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the first CSV data\n",
    "domain_index1 = train_data_csv1.columns.get_loc(\"Domain_ori\")\n",
    "position_index1 = train_data_csv1.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "train_features_csv1 = train_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "train_target_csv1 = train_data_csv1['quality(q^2)']\n",
    "test_features_csv1 = test_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "test_target_csv1 = test_data_csv1['quality(q^2)']\n",
    "\n",
    "# Fill missing values with -1 for the second CSV data\n",
    "data_csv2.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the second CSV data\n",
    "domain_index2 = data_csv2.columns.get_loc(\"Domain_ori\")\n",
    "position_index2 = data_csv2.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "features_csv2 = data_csv2.iloc[:, domain_index2:position_index2]\n",
    "target_csv2 = data_csv2['quality(q^2)']\n",
    "\n",
    "# Split the second CSV data into training and testing sets\n",
    "train_features_csv2, test_features_csv2, train_target_csv2, test_target_csv2 = train_test_split(\n",
    "    features_csv2, target_csv2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv1, train_features_csv2], ignore_index=True)\n",
    "train_target_combined = pd.concat([train_target_csv1, train_target_csv2], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv1, test_features_csv2], ignore_index=True)\n",
    "test_target_combined = pd.concat([test_target_csv1, test_target_csv2], ignore_index=True)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "rf_model.fit(train_features_combined, train_target_combined)\n",
    "\n",
    "# Make predictions on the combined test set\n",
    "predictions = rf_model.predict(test_features_combined)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_target_combined, predictions)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99998894",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_csv1 = rf_model.predict(test_features_csv1)\n",
    "\n",
    "for i, (prediction, actual) in enumerate(zip(predictions_csv1, test_target_csv1)):\n",
    "    print(f\"Sample {i+1} - Predicted quality(q^2): {prediction}, Actual quality(q^2): {actual}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0715e1",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993cedf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_german_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_german.csv\")\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv = data_csv[~data_csv['experiment'].isin(['ESS4 political_trust', 'ESS7 subjective_competence'])]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(['ESS4 political_trust', 'ESS7 subjective_competence'])]\n",
    "\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for CSV data\n",
    "train_data_csv = train_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data_csv = test_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for Excel data\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "features_excel = data_excel[[\"Request for answer text\", \"Answer options text\"]]\n",
    "labels_excel = data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Split Excel data into training and testing sets\n",
    "train_features_excel, test_features_excel, train_labels_excel, test_labels_excel = train_test_split(\n",
    "    features_excel, labels_excel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4fdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a9c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to preprocess the text based on 'Introduction text' length\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_german_manual.csv\", encoding='unicode_escape')\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_german.csv\", encoding='unicode_escape')\n",
    "\n",
    "# Apply preprocessing to the CSV data\n",
    "data_csv['Processed Request for answer text'] = data_csv.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Apply preprocessing to the Excel data\n",
    "data_excel['Processed Request for answer text'] = data_excel.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv = data_csv[~data_csv['experiment'].isin(['ESS1 political_efficacy'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(['ESS1 political_efficacy'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for CSV data\n",
    "train_data_csv = train_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "test_data_csv = test_data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column for Excel data\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "features_excel = data_excel[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "labels_excel = data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Split Excel data into training and testing sets\n",
    "train_features_excel, test_features_excel, train_labels_excel, test_labels_excel = train_test_split(\n",
    "    features_excel, labels_excel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f364c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_csv = QualityDataset(tokenizer, (\n",
    "    test_features_csv[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_csv[\"Answer options text\"].tolist()), test_labels_csv.tolist())\n",
    "\n",
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(test_dataset_csv)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "mse = mean_squared_error(test_labels_csv, predicted_labels)\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "output_df = test_features_csv.copy()\n",
    "output_df['actual_quality'] = test_labels_csv\n",
    "output_df['predicted_quality'] = predicted_labels\n",
    "output_df.to_csv('./group_results/german/predictionsESS1 political_efficacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f0e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from both CSV files\n",
    "data_csv2 = pd.read_csv(\"final_exp_nongroup_german.csv\", encoding='unicode_escape')\n",
    "data_csv1 = pd.read_csv(\"final_exp_group_german_manual.csv\", encoding='unicode_escape')\n",
    "\n",
    "# # Filter the data for training and testing based on 'study' column from the first CSV\n",
    "# train_data_csv1 = data_csv1[data_csv1['Study'].isin(['ESS Round 1', 'ESS Round 2', 'ESS Round 3', 'ESS Round 4','ESS Round 7'])]\n",
    "# test_data_csv1 = data_csv1[data_csv1['Study'] == 'ESS Round 5']\n",
    "\n",
    "# Filter the data based on 'experiment' column for training and testing\n",
    "train_data_csv1 = data_csv1[~data_csv1['experiment'].isin(['ESS1 political_efficacy'])] #,'ESS1 political_efficacy' 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "test_data_csv1 = data_csv1[data_csv1['experiment'].isin(['ESS1 political_efficacy'])] #, 'ESS7 subjective_competence','ESS4 political_trust'])]\n",
    "\n",
    "\n",
    "# Fill missing values with -1 for the first CSV data\n",
    "train_data_csv1.fillna(-1, inplace=True)\n",
    "test_data_csv1.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the first CSV data\n",
    "domain_index1 = train_data_csv1.columns.get_loc(\"Domain_ori\")\n",
    "position_index1 = train_data_csv1.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "train_features_csv1 = train_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "train_target_csv1 = train_data_csv1['quality(q^2)']\n",
    "test_features_csv1 = test_data_csv1.iloc[:, domain_index1:position_index1]\n",
    "test_target_csv1 = test_data_csv1['quality(q^2)']\n",
    "\n",
    "# Fill missing values with -1 for the second CSV data\n",
    "data_csv2.fillna(-1, inplace=True)\n",
    "\n",
    "# Extract features and target for the second CSV data\n",
    "domain_index2 = data_csv2.columns.get_loc(\"Domain_ori\")\n",
    "position_index2 = data_csv2.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\")+1\n",
    "\n",
    "features_csv2 = data_csv2.iloc[:, domain_index2:position_index2]\n",
    "target_csv2 = data_csv2['quality(q^2)']\n",
    "\n",
    "# Split the second CSV data into training and testing sets\n",
    "train_features_csv2, test_features_csv2, train_target_csv2, test_target_csv2 = train_test_split(\n",
    "    features_csv2, target_csv2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv1, train_features_csv2], ignore_index=True)\n",
    "train_target_combined = pd.concat([train_target_csv1, train_target_csv2], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv1, test_features_csv2], ignore_index=True)\n",
    "test_target_combined = pd.concat([test_target_csv1, test_target_csv2], ignore_index=True)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "rf_model.fit(train_features_combined, train_target_combined)\n",
    "\n",
    "# Make predictions on the combined test set\n",
    "predictions = rf_model.predict(test_features_combined)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_target_combined, predictions)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_csv1 = rf_model.predict(test_features_csv1)\n",
    "\n",
    "for i, (prediction, actual) in enumerate(zip(predictions_csv1, test_target_csv1)):\n",
    "    print(f\"Sample {i+1} - Predicted quality(q^2): {prediction}, Actual quality(q^2): {actual}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a977a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e51a20",
   "metadata": {},
   "source": [
    "# all data - experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa0576",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa451d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to preprocess the text based on 'Introduction text' length\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "\n",
    "# Apply preprocessing to the CSV data\n",
    "data_csv['Processed Request for answer text'] = data_csv.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Apply preprocessing to the Excel data\n",
    "data_excel['Processed Request for answer text'] = data_excel.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Separate out ESS1 political_efficacy as the final validation set from CSV data\n",
    "validation_data_csv = data_csv[data_csv['experiment'] == 'ESS1 political_efficacy']\n",
    "data_csv = data_csv[data_csv['experiment'] != 'ESS1 political_efficacy']  # Remaining data for train/test split\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column\n",
    "data_csv = data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Split CSV data based on unique 'experiment' groups for training and testing\n",
    "unique_experiments = data_csv['experiment'].unique()\n",
    "train_experiments_csv, test_experiments_csv = train_test_split(unique_experiments, test_size=0.2, random_state=42)\n",
    "train_data_csv = data_csv[data_csv['experiment'].isin(train_experiments_csv)]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(test_experiments_csv)]\n",
    "\n",
    "# Split Excel data based on unique 'study' groups for training and testing\n",
    "unique_studies = data_excel['Study'].unique()\n",
    "train_studies_excel, test_studies_excel = train_test_split(unique_studies, test_size=0.2, random_state=42)\n",
    "train_data_excel = data_excel[data_excel['Study'].isin(train_studies_excel)]\n",
    "test_data_excel = data_excel[data_excel['Study'].isin(test_studies_excel)]\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv[[\"Study\", \"Country\",  \"ItemConcept\",\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv[[\"Study\", \"Country\",  \"ItemConcept\",\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "train_features_excel = train_data_excel[[\"Study\", \"Country\",  \"ItemConcept\",\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "train_labels_excel = train_data_excel[\"quality(q^2)\"]\n",
    "test_features_excel = test_data_excel[[\"Study\", \"Country\",  \"ItemConcept\",\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "test_labels_excel = test_data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Prepare the validation set features and labels\n",
    "validation_features_csv = validation_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "validation_labels_csv = validation_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n",
    "\n",
    "validation_dataset = QualityDataset(tokenizer, (\n",
    "    validation_features_csv[\"Processed Request for answer text\"].tolist(), \n",
    "    validation_features_csv[\"Answer options text\"].tolist()), validation_labels_csv.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to preprocess the text based on 'Introduction text' length\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "# Load the data from CSV\n",
    "data_csv = pd.read_csv(\"final_exp_group_eng_manual.csv\")\n",
    "\n",
    "# Load the data from Excel\n",
    "data_excel = pd.read_csv(\"final_exp_nongroup_eng.csv\")\n",
    "\n",
    "# Apply preprocessing to the CSV data\n",
    "data_csv['Processed Request for answer text'] = data_csv.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Apply preprocessing to the Excel data\n",
    "data_excel['Processed Request for answer text'] = data_excel.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Separate out ESS1 political_efficacy as the final validation set from CSV data\n",
    "validation_data_csv = data_csv[data_csv['experiment'] == 'ESS1 political_efficacy']\n",
    "data_csv = data_csv[data_csv['experiment'] != 'ESS1 political_efficacy']  # Remaining data for train/test split\n",
    "\n",
    "# Ensure no missing values in the 'quality(q^2)' column\n",
    "data_csv = data_csv.dropna(subset=[\"quality(q^2)\"])\n",
    "data_excel = data_excel.dropna(subset=[\"quality(q^2)\"])\n",
    "\n",
    "# Split CSV data based on unique 'experiment' groups for training and testing\n",
    "unique_experiments = data_csv['experiment'].unique()\n",
    "train_experiments_csv, test_experiments_csv = train_test_split(unique_experiments, test_size=0.2, random_state=42)\n",
    "train_data_csv = data_csv[data_csv['experiment'].isin(train_experiments_csv)]\n",
    "test_data_csv = data_csv[data_csv['experiment'].isin(test_experiments_csv)]\n",
    "\n",
    "# Split Excel data based on unique 'study' groups for training and testing\n",
    "unique_studies = data_excel['Study'].unique()\n",
    "train_studies_excel, test_studies_excel = train_test_split(unique_studies, test_size=0.2, random_state=42)\n",
    "train_data_excel = data_excel[data_excel['Study'].isin(train_studies_excel)]\n",
    "test_data_excel = data_excel[data_excel['Study'].isin(test_studies_excel)]\n",
    "\n",
    "# Extract features and labels for CSV data\n",
    "train_features_csv = train_data_csv\n",
    "train_labels_csv = train_data_csv[\"quality(q^2)\"]\n",
    "test_features_csv = test_data_csv\n",
    "test_labels_csv = test_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Extract features and labels for Excel data\n",
    "train_features_excel = train_data_excel\n",
    "train_labels_excel = train_data_excel[\"quality(q^2)\"]\n",
    "test_features_excel = test_data_excel\n",
    "test_labels_excel = test_data_excel[\"quality(q^2)\"]\n",
    "\n",
    "# Combine the training data from both sources\n",
    "train_features_combined = pd.concat([train_features_csv, train_features_excel], ignore_index=True)\n",
    "train_labels_combined = pd.concat([train_labels_csv, train_labels_excel], ignore_index=True)\n",
    "\n",
    "# Combine the testing data from both sources\n",
    "test_features_combined = pd.concat([test_features_csv, test_features_excel], ignore_index=True)\n",
    "test_labels_combined = pd.concat([test_labels_csv, test_labels_excel], ignore_index=True)\n",
    "\n",
    "# Prepare the validation set features and labels\n",
    "validation_features_csv = validation_data_csv[[\"Processed Request for answer text\", \"Answer options text\"]]\n",
    "validation_labels_csv = validation_data_csv[\"quality(q^2)\"]\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the combined datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "    train_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    train_features_combined[\"Answer options text\"].tolist()), train_labels_combined.tolist())\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "    test_features_combined[\"Processed Request for answer text\"].tolist(), \n",
    "    test_features_combined[\"Answer options text\"].tolist()), test_labels_combined.tolist())\n",
    "\n",
    "validation_dataset = QualityDataset(tokenizer, (\n",
    "    validation_features_csv[\"Processed Request for answer text\"].tolist(), \n",
    "    validation_features_csv[\"Answer options text\"].tolist()), validation_labels_csv.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_combined = train_features_combined.copy()\n",
    "train_combined[\"quality(q^2)\"] = train_labels_combined\n",
    "\n",
    "test_combined = test_features_combined.copy()\n",
    "test_combined[\"quality(q^2)\"] = test_labels_combined\n",
    "\n",
    "# train_combined.to_csv(\"train_combined.csv\", index=False)\n",
    "# test_combined.to_csv(\"test_combined.csv\", index=False)\n",
    "\n",
    "train_combined.to_csv(\"./data_allgroup_rf/train.csv\", index=False)\n",
    "test_combined.to_csv(\"./data_allgroup_rf/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a768f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_csv = QualityDataset(tokenizer, (\n",
    "#     test_features_csv[\"Processed Request for answer text\"].tolist(), \n",
    "#     test_features_csv[\"Answer options text\"].tolist()), test_labels_csv.tolist())\n",
    "\n",
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(validation_dataset)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "mse = mean_squared_error(validation_labels_csv, predicted_labels)\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "output_df = validation_features_csv.copy()\n",
    "output_df['actual_quality'] = validation_labels_csv\n",
    "output_df['predicted_quality'] = predicted_labels\n",
    "output_df.to_csv('./group_results_allrandom/predictionsESS1 political_efficacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c85841",
   "metadata": {},
   "source": [
    "# Test how models perform on orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca772692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "checkpoint = \"results_final/checkpoint-3600-1616\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_final',  \n",
    "    logging_dir='./logs',          \n",
    "    do_train=False,                \n",
    "    do_eval=True,                \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined[\"predicted_quality(q^2)\"] = predicted_labels\n",
    "\n",
    "\n",
    "train_combined.to_csv(\"./data_allgroup/train_data_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "test_combined[\"predicted_quality(q^2)\"] = predicted_labels\n",
    "\n",
    "\n",
    "test_combined.to_csv(\"./data_allgroup/test_data_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b194b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the test dataset with predictions\n",
    "df = test_combined\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "\n",
    "def cal_correctpairs(df_sorted):\n",
    "    # Initialize pair tracking\n",
    "    correct_predictions = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    # Compare all pairwise combinations within each group\n",
    "    for _, group in df_sorted.groupby([\"Study\", \"Country\", \"ItemConcept\"]):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        group = group.reset_index(drop=True)\n",
    "\n",
    "        for i, j in combinations(range(len(group)), 2):\n",
    "            total_pairs += 1\n",
    "\n",
    "            true_i = group.loc[i, \"quality(q^2)\"]\n",
    "            true_j = group.loc[j, \"quality(q^2)\"]\n",
    "\n",
    "            pred_i = group.loc[i, \"predicted_quality(q^2)\"]\n",
    "            pred_j = group.loc[j, \"predicted_quality(q^2)\"]\n",
    "\n",
    "            true_diff = true_i - true_j\n",
    "            pred_diff = pred_i - pred_j\n",
    "\n",
    "            if (true_diff > 0 and pred_diff > 0) or (true_diff < 0 and pred_diff < 0) or (true_diff == 0 and abs(pred_diff) < 1e-6):\n",
    "                correct_predictions += 1\n",
    "                \n",
    "    return total_pairs,correct_predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset with predictions\n",
    "df = train_combined\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f7ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "train_df = pd.read_csv(\"./data_allgroup_rf/train.csv\")\n",
    "test_df = pd.read_csv(\"./data_allgroup_rf/test.csv\")\n",
    "\n",
    "train_df.fillna(-1, inplace=True)\n",
    "test_df.fillna(-1, inplace=True)\n",
    "\n",
    "domain_index = train_df.columns.get_loc(\"Domain_ori\")\n",
    "position_index = train_df.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\") + 1\n",
    "\n",
    "X_train = train_df.iloc[:, domain_index:position_index]\n",
    "y_train = train_df[\"quality(q^2)\"]\n",
    "X_test = test_df.iloc[:, domain_index:position_index]\n",
    "y_test = test_df[\"quality(q^2)\"]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_df[\"predicted_quality(q^2)\"] = model.predict(X_train)\n",
    "test_df[\"predicted_quality(q^2)\"] = model.predict(X_test)\n",
    "\n",
    "train_df.to_csv(\"./data_allgroup_rf/train_with_predictions.csv\", index=False)\n",
    "test_df.to_csv(\"./data_allgroup_rf/test_with_predictions.csv\", index=False)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, test_df[\"predicted_quality(q^2)\"])\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d23b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the test dataset with predictions\n",
    "df = test_df\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "\n",
    "def cal_correctpairs(df_sorted):\n",
    "    # Initialize pair tracking\n",
    "    correct_predictions = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    # Compare all pairwise combinations within each group\n",
    "    for _, group in df_sorted.groupby([\"Study\", \"Country\", \"ItemConcept\"]):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        group = group.reset_index(drop=True)\n",
    "\n",
    "        for i, j in combinations(range(len(group)), 2):\n",
    "            total_pairs += 1\n",
    "\n",
    "            true_i = group.loc[i, \"quality(q^2)\"]\n",
    "            true_j = group.loc[j, \"quality(q^2)\"]\n",
    "\n",
    "            pred_i = group.loc[i, \"predicted_quality(q^2)\"]\n",
    "            pred_j = group.loc[j, \"predicted_quality(q^2)\"]\n",
    "\n",
    "            true_diff = true_i - true_j\n",
    "            pred_diff = pred_i - pred_j\n",
    "\n",
    "            if (true_diff > 0 and pred_diff > 0) or (true_diff < 0 and pred_diff < 0) or (true_diff == 0 and abs(pred_diff) < 1e-6):\n",
    "                correct_predictions += 1\n",
    "                \n",
    "    return total_pairs,correct_predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset with predictions\n",
    "df = train_df\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06522caa",
   "metadata": {},
   "source": [
    "# All data split by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df_cleaned = pd.read_excel(\"SQP_dummyvars_data.xlsx\")\n",
    "df_cleaned[\"ItemConcept\"] = df_cleaned[\"ItemConcept\"].apply(lambda x: re.sub(r'\\s*\\(method\\s*\\d+\\)', '', str(x)).strip())\n",
    "\n",
    "# Recreate the group_id based on cleaned ItemConcept\n",
    "df_cleaned[\"group_id\"] = df_cleaned.groupby([\"Study\", \"Country\", \"ItemConcept\"]).ngroup() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e846ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"cleaned_full_grouped_sqp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a80c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned file with group_id\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Randomly select 80% of the group_ids for training\n",
    "unique_ids = df[\"group_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the dataframe based on selected group_ids\n",
    "train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "# Fill missing values\n",
    "train_df.fillna(-1, inplace=True)\n",
    "test_df.fillna(-1, inplace=True)\n",
    "\n",
    "# Identify feature column range\n",
    "domain_index = train_df.columns.get_loc(\"Domain_ori\")\n",
    "position_index = train_df.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\") + 1\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_df.iloc[:, domain_index:position_index]\n",
    "y_train = train_df[\"quality(q^2)\"]\n",
    "X_test = test_df.iloc[:, domain_index:position_index]\n",
    "y_test = test_df[\"quality(q^2)\"]\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "train_df[\"predicted_quality(q^2)\"] = model.predict(X_train)\n",
    "test_df[\"predicted_quality(q^2)\"] = model.predict(X_test)\n",
    "\n",
    "# Compute and display test MSE\n",
    "mse = mean_squared_error(y_test, test_df[\"predicted_quality(q^2)\"])\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"data_allgroup_split/test_data_with_predictions_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the test dataset with predictions\n",
    "df = test_df\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "\n",
    "def cal_correctpairs(df_sorted):\n",
    "    # Initialize pair tracking\n",
    "    correct_predictions = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    # Compare all pairwise combinations within each group\n",
    "    for _, group in df_sorted.groupby([\"Study\", \"Country\", \"ItemConcept\"]):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        group = group.reset_index(drop=True)\n",
    "\n",
    "        for i, j in combinations(range(len(group)), 2):\n",
    "            total_pairs += 1\n",
    "\n",
    "            true_i = group.loc[i, \"quality(q^2)\"]\n",
    "            true_j = group.loc[j, \"quality(q^2)\"]\n",
    "\n",
    "            pred_i = group.loc[i, \"predicted_quality(q^2)\"]\n",
    "            pred_j = group.loc[j, \"predicted_quality(q^2)\"]\n",
    "\n",
    "            true_diff = true_i - true_j\n",
    "            pred_diff = pred_i - pred_j\n",
    "\n",
    "            if (true_diff > 0 and pred_diff > 0) or (true_diff < 0 and pred_diff < 0) or (true_diff == 0 and abs(pred_diff) < 1e-6):\n",
    "                correct_predictions += 1\n",
    "                \n",
    "    return total_pairs,correct_predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset with predictions\n",
    "df = train_df\n",
    "\n",
    "# Function to extract numerical part for proper sorting\n",
    "def extract_numeric_part(item):\n",
    "    match = re.search(r'([A-Za-z]+)(\\d+)', str(item))\n",
    "    if match:\n",
    "        return match.group(1), int(match.group(2))\n",
    "    return item, float('inf')\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(\n",
    "    by=[\"Study\", \"Country\", \"ItemConcept\"],\n",
    "    key=lambda x: x.map(lambda y: extract_numeric_part(y)) if x.name == \"ItemAdmin\" else x\n",
    ")\n",
    "# Calculate accuracy\n",
    "total_pairs,correct_predictions = cal_correctpairs(df_sorted)\n",
    "accuracy = correct_predictions / total_pairs if total_pairs > 0 else None\n",
    "accuracy, total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c20d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "checkpoint = \"results_roberta_splitbygroup_newdata/checkpoint-8400\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_final',  \n",
    "    logging_dir='./logs',          \n",
    "    do_train=False,                \n",
    "    do_eval=True,                \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c810a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./data_allgroup_split_newdata2/train_combined.csv\")\n",
    "train_combined = pd.read_csv(\"./data_allgroup_split_newdata2/train_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0266a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"./data_allgroup_split_newdata2/test_combined.csv\")\n",
    "test_combined = pd.read_csv(\"./data_allgroup_split_newdata2/test_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined[\"predicted_quality(q^2)\"] = predicted_labels\n",
    "\n",
    "\n",
    "train_combined.to_csv(\"./data_allgroup_split_newdata2/train_data_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08da56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trainer to predict on the test data CSV\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = predictions.predictions.squeeze()\n",
    "\n",
    "test_combined[\"predicted_quality(q^2)\"] = predicted_labels\n",
    "\n",
    "\n",
    "test_combined.to_csv(\"./data_allgroup_split_newdata2/test_data_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "# Load the cleaned file with group_id\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Randomly select 80% of the group_ids for training\n",
    "unique_ids = df[\"group_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the dataframe based on selected group_ids\n",
    "train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "# Reuse train_df and test_df from earlier training session\n",
    "# We'll create a new script structure using these for the Roberta model\n",
    "\n",
    "# Preprocess text combination logic\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['Processed Request for answer text'] = train_df.apply(preprocess_text, axis=1)\n",
    "test_df['Processed Request for answer text'] = test_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Define HuggingFace Dataset wrapper\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QualityDataset(tokenizer,\n",
    "    (train_df[\"Processed Request for answer text\"].tolist(), train_df[\"Answer options text\"].tolist()),\n",
    "    train_df[\"quality(q^2)\"].tolist()\n",
    ")\n",
    "\n",
    "test_dataset = QualityDataset(tokenizer,\n",
    "    (test_df[\"Processed Request for answer text\"].tolist(), test_df[\"Answer options text\"].tolist()),\n",
    "    test_df[\"quality(q^2)\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # This is the finalized structure for the new data setup\n",
    "# final_training_setup_code = {\n",
    "#     \"train_df_rows\": len(train_df),\n",
    "#     \"test_df_rows\": len(test_df),\n",
    "#     \"sample_processed_text\": train_df[\"Processed Request for answer text\"].iloc[0][:100]\n",
    "# }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments 2025-11-17\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_roberta_splitbygroup_newdata',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.005,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine_with_restarts',\n",
    "    save_total_limit=30,\n",
    "    save_steps=300,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b6bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training arguments 2025-05-21\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_roberta_splitbygroup',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.005,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine_with_restarts',\n",
    "    save_total_limit=30,\n",
    "    save_steps=300,\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c63c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_roberta_splitbygroup',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=10\n",
    ")\n",
    "\n",
    "# MSE metric\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9513f47",
   "metadata": {},
   "source": [
    "# CI for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd9622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned file with group_id\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify feature column range\n",
    "domain_index = df.columns.get_loc(\"Domain_ori\")\n",
    "position_index = df.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\") + 1\n",
    "\n",
    "# Number of repeats\n",
    "n_repeats = 100\n",
    "mse_list = []\n",
    "\n",
    "# Repeated holdout validation\n",
    "for i in range(n_repeats):\n",
    "    print(i)\n",
    "    # Randomly split group_ids\n",
    "    unique_ids = df[\"group_id\"].unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=None)\n",
    "    \n",
    "    # Split dataframe\n",
    "    train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "    test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Fill missing values\n",
    "    train_df.fillna(-1, inplace=True)\n",
    "    test_df.fillna(-1, inplace=True)\n",
    "\n",
    "    # Extract features and target\n",
    "    X_train = train_df.iloc[:, domain_index:position_index]\n",
    "    y_train = train_df[\"quality(q^2)\"]\n",
    "    X_test = test_df.iloc[:, domain_index:position_index]\n",
    "    y_test = test_df[\"quality(q^2)\"]\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Convert to numpy array\n",
    "mse_array = np.array(mse_list)\n",
    "\n",
    "# Calculate 95% confidence interval and mean\n",
    "mean_mse = np.mean(mse_array)\n",
    "lower_ci = np.percentile(mse_array, 2.5)\n",
    "upper_ci = np.percentile(mse_array, 97.5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Repeated Holdout Validation Results ({n_repeats} runs):\")\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval for MSE: [{lower_ci:.4f}, {upper_ci:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce174fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(f\"Repeated Holdout Validation Results ({n_repeats} runs):\")\n",
    "print(f\"Mean MSE: {mean_mse:.5f}\")\n",
    "print(f\"95% Confidence Interval for MSE: [{lower_ci:.5f}, {upper_ci:.5f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b25d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned file with group_id\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify feature column range\n",
    "domain_index = df.columns.get_loc(\"Domain_ori\")\n",
    "position_index = df.columns.get_loc(\"Position_ori\") + 1\n",
    "\n",
    "# Number of repeats\n",
    "n_repeats = 69\n",
    "# mse_list = []\n",
    "\n",
    "# Repeated holdout validation\n",
    "for i in range(n_repeats):\n",
    "    print(i)\n",
    "    # Randomly split group_ids\n",
    "    unique_ids = df[\"group_id\"].unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=None)\n",
    "    \n",
    "    # Split dataframe\n",
    "    train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "    test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Fill missing values\n",
    "    train_df.fillna(-1, inplace=True)\n",
    "    test_df.fillna(-1, inplace=True)\n",
    "\n",
    "    # Extract features and target\n",
    "    X_train = train_df.iloc[:, domain_index:position_index]\n",
    "    y_train = train_df[\"quality(q^2)\"]\n",
    "    X_test = test_df.iloc[:, domain_index:position_index]\n",
    "    y_test = test_df[\"quality(q^2)\"]\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and calculate MSE\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Convert to numpy array\n",
    "mse_array_with_external = np.array(mse_list)\n",
    "\n",
    "# Calculate 95% confidence interval and mean\n",
    "mean_mse = np.mean(mse_array_with_external)\n",
    "lower_ci = np.percentile(mse_array_with_external, 2.5)\n",
    "upper_ci = np.percentile(mse_array_with_external, 97.5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Repeated Holdout Validation Results ({n_repeats} runs):\")\n",
    "print(f\"Mean MSE: {mean_mse:.5f}\")\n",
    "print(f\"95% Confidence Interval for MSE: [{lower_ci:.5f}, {upper_ci:.5f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_array_no_external = mse_array.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ed5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_array_no_external"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e0d2b",
   "metadata": {},
   "source": [
    "# Perform Pair Evaluation and Wilcoxon Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5fdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Load data\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "n_repeats = 100\n",
    "mse_with_external = []\n",
    "mse_without_external = []\n",
    "\n",
    "# Get domain_index\n",
    "domain_index = df.columns.get_loc(\"Domain_ori\")\n",
    "\n",
    "for i in range(n_repeats):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Random group_id split\n",
    "    unique_ids = df[\"group_id\"].unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=None)\n",
    "\n",
    "    train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "    test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "    train_df.fillna(-1, inplace=True)\n",
    "    test_df.fillna(-1, inplace=True)\n",
    "\n",
    "    # ---- Model A: with external features ----\n",
    "    pos_index_A = df.columns.get_loc(\"Visual or oral presentation_ori\") + 1\n",
    "    X_train_A = train_df.iloc[:, domain_index:pos_index_A]\n",
    "    X_test_A = test_df.iloc[:, domain_index:pos_index_A]\n",
    "\n",
    "    model_A = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "    model_A.fit(X_train_A, train_df[\"quality(q^2)\"])\n",
    "    pred_A = model_A.predict(X_test_A)\n",
    "    mse_A = mean_squared_error(test_df[\"quality(q^2)\"], pred_A)\n",
    "    mse_with_external.append(mse_A)\n",
    "\n",
    "    # ---- Model B: without external features ----\n",
    "    pos_index_B = df.columns.get_loc(\"Total number of abstract nouns in answer scale_ori\") + 1\n",
    "    X_train_B = train_df.iloc[:, domain_index:pos_index_B]\n",
    "    X_test_B = test_df.iloc[:, domain_index:pos_index_B]\n",
    "\n",
    "    model_B = RandomForestRegressor(n_estimators=520, max_depth=50, max_features=16, random_state=42)\n",
    "    model_B.fit(X_train_B, train_df[\"quality(q^2)\"])\n",
    "    pred_B = model_B.predict(X_test_B)\n",
    "    mse_B = mean_squared_error(test_df[\"quality(q^2)\"], pred_B)\n",
    "    mse_without_external.append(mse_B)\n",
    "\n",
    "# ---- Convert to NumPy for CI calculation ----\n",
    "mse_with_external = np.array(mse_with_external)\n",
    "mse_without_external = np.array(mse_without_external)\n",
    "\n",
    "# ---- Wilcoxon signed-rank test ----\n",
    "stat, p_value = wilcoxon(mse_with_external, mse_without_external)\n",
    "print(f\"\\nWilcoxon signed-rank test p-value: {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in MSE is statistically significant.\")\n",
    "else:\n",
    "    print(\"No significant difference in MSE was found.\")\n",
    "\n",
    "# ---- Print mean and 95% CI for both models ----\n",
    "mean_A = np.mean(mse_with_external)\n",
    "ci_A = np.percentile(mse_with_external, [2.5, 97.5])\n",
    "\n",
    "mean_B = np.mean(mse_without_external)\n",
    "ci_B = np.percentile(mse_without_external, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\nModel A (with external features):\")\n",
    "print(f\"  Mean MSE = {mean_A:.4f}\")\n",
    "print(f\"  95% Confidence Interval = [{ci_A[0]:.4f}, {ci_A[1]:.4f}]\")\n",
    "\n",
    "print(f\"\\nModel B (without external features):\")\n",
    "print(f\"  Mean MSE = {mean_B:.4f}\")\n",
    "print(f\"  95% Confidence Interval = [{ci_B[0]:.4f}, {ci_B[1]:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_with_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8531a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_without_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e707d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6396168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nModel A (with external features):\")\n",
    "print(f\"  Mean MSE = {mean_A}\")\n",
    "print(f\"  95% Confidence Interval = [{ci_A[0]}, {ci_A[1]}]\")\n",
    "\n",
    "print(f\"\\nModel B (without external features):\")\n",
    "print(f\"  Mean MSE = {mean_B}\")\n",
    "print(f\"  95% Confidence Interval = [{ci_B[0]}, {ci_B[1]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aeebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926c539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa33c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f215e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2d72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a627c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file1 = \"./olddata/SQP_dummyvars_data.xlsx\"\n",
    "file2 = \"SQP_dummyvars_data.xlsx\"\n",
    "\n",
    "df1 = pd.read_excel(file1)\n",
    "df2 = pd.read_excel(file2)\n",
    "\n",
    "print(\"File1 shape:\", df1.shape)\n",
    "print(\"File2 shape:\", df2.shape)\n",
    "\n",
    "def count_language(df, name):\n",
    "    if \"Language\" in df.columns:\n",
    "        print(f\"\\n=== Language counts in {name} ===\")\n",
    "        print(df[\"Language\"].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(f\"\\n{name} does NOT contain a 'Language' column.\")\n",
    "\n",
    "count_language(df1, \"df1\")\n",
    "count_language(df2, \"df2\")\n",
    "\n",
    "\n",
    "def plot_quality(df, name):\n",
    "    col_name = \"quality(q^2)\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"{name} does NOT contain '{col_name}' column.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    data = df[col_name].dropna()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.violinplot(y=data)\n",
    "    plt.title(f\"Violin Plot of quality(q^2) - {name}\")\n",
    "    plt.ylabel(\"quality(q^2)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_quality(df1, \"df1\")\n",
    "plot_quality(df2, \"df2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0af534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"old: {len(df1)} rows\")\n",
    "print(f\"new: {len(df2)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"quality(q^2)\"\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df1[col], kde=True, stat=\"density\", label=\"df1\", alpha=0.5)\n",
    "sns.histplot(df2[col], kde=True, stat=\"density\", label=\"df2\", alpha=0.5)\n",
    "plt.title(\"Histogram + KDE Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer, Trainer, TrainingArguments, PreTrainedModel,XLMRobertaForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "file_path = \"cleaned_full_grouped_sqp.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split by group_id\n",
    "unique_ids = df[\"group_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_df = df[df[\"group_id\"].isin(train_ids)].copy()\n",
    "test_df = df[df[\"group_id\"].isin(test_ids)].copy()\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(row):\n",
    "    if len(str(row['Introduction text'])) > 10:\n",
    "        return row['Introduction text'] + \" \" + row['Request for answer text']\n",
    "    else:\n",
    "        return \"</no introduction text/> \" + row['Request for answer text']\n",
    "\n",
    "train_df['Processed Request for answer text'] = train_df.apply(preprocess_text, axis=1)\n",
    "test_df['Processed Request for answer text'] = test_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "# features = data[[\"Request for answer text\", \"Answer options text\"]]\n",
    "# # features = data[[\"Introduction text\",\"Request for answer text\", \"Answer options text\"]]\n",
    "# labels = data[\"quality(q^2)\"]\n",
    "\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "train_features = train_df[[\"Request for answer text\", \"Answer options text\"]]\n",
    "test_features = test_df[[\"Request for answer text\", \"Answer options text\"]]\n",
    "train_labels = train_df[\"quality(q^2)\"]\n",
    "test_labels = test_df[\"quality(q^2)\"]\n",
    "\n",
    "# Define a dataset class\n",
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels=None):\n",
    "#         new_txt = [str(a)+\" \"+b for a,b in zip(texts[0],texts[1])]\n",
    "        self.encodings = tokenizer(texts[0], texts[1], truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = QualityDataset(tokenizer, (\n",
    "#     train_features[\"Introduction text\"].tolist(),\n",
    "    train_features[\"Request for answer text\"].tolist(), \n",
    "    train_features[\"Answer options text\"].tolist()), train_labels.tolist())\n",
    "test_dataset = QualityDataset(tokenizer, (\n",
    "#     test_features[\"Introduction text\"].tolist(),\n",
    "    test_features[\"Request for answer text\"].tolist(), \n",
    "    test_features[\"Answer options text\"].tolist()), test_labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments 2025-11-17\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_final_newdata',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.005,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    lr_scheduler_type='cosine_with_restarts',\n",
    "    save_total_limit=30,\n",
    "    save_steps=300,\n",
    ")\n",
    "\n",
    "# Define MSE computation for evaluation\n",
    "def compute_mse(p):\n",
    "    return {\"mse\": mean_squared_error(p.label_ids, p.predictions.squeeze())}\n",
    "\n",
    "# Load the model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_mse,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b922b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
